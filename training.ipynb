{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, Sampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from pytorch_pretrained_bert import BertAdam\n",
    "from pytorch_pretrained_bert import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/bert_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ko_bt</th>\n",
       "      <th>org_ids</th>\n",
       "      <th>ko_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>One of the other reviewers mentioned that you ...</td>\n",
       "      <td>[101, 2028, 1997, 1996, 2060, 15814, 2038, 385...</td>\n",
       "      <td>[101, 2028, 1997, 1996, 2060, 15814, 3855, 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Nice little production. &lt;br /&gt;&lt;br /&gt; The shoot...</td>\n",
       "      <td>[101, 1037, 6919, 2210, 2537, 1012, 1026, 7987...</td>\n",
       "      <td>[101, 3835, 2210, 2537, 1012, 1026, 7987, 1013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I thought this was a great way to spend time o...</td>\n",
       "      <td>[101, 1045, 2245, 2023, 2001, 1037, 6919, 2126...</td>\n",
       "      <td>[101, 1045, 2245, 2023, 2001, 1037, 2307, 2126...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Basically there is a family where a little boy...</td>\n",
       "      <td>[101, 10468, 2045, 1005, 1055, 1037, 2155, 207...</td>\n",
       "      <td>[101, 10468, 2045, 2003, 1037, 2155, 2073, 103...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Petter Mattei&amp;#39;s &amp;quot;Love for Money&amp;#39;s...</td>\n",
       "      <td>[101, 9004, 3334, 4717, 7416, 1005, 1055, 1000...</td>\n",
       "      <td>[101, 9004, 3334, 4717, 7416, 1004, 1001, 4464...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Perhaps the story of the best film ever, selfl...</td>\n",
       "      <td>[101, 2763, 2026, 2035, 1011, 2051, 5440, 3185...</td>\n",
       "      <td>[101, 3383, 1996, 2466, 1997, 1996, 2190, 2143...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I would like to see the revival of the Seahunt...</td>\n",
       "      <td>[101, 1045, 2469, 2052, 2066, 2000, 2156, 1037...</td>\n",
       "      <td>[101, 1045, 2052, 2066, 2000, 2156, 1996, 6308...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "      <td>This show was a stunning fresh and innovative ...</td>\n",
       "      <td>[101, 2023, 2265, 2001, 2019, 6429, 1010, 4840...</td>\n",
       "      <td>[101, 2023, 2265, 2001, 1037, 14726, 4840, 199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "      <td>I was looking forward to seeing this movie bec...</td>\n",
       "      <td>[101, 6628, 2011, 1996, 3893, 7928, 2055, 2023...</td>\n",
       "      <td>[101, 1045, 2001, 2559, 2830, 2000, 3773, 2023...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>If you like good old-fashioned laughs, you&amp;#39...</td>\n",
       "      <td>[101, 2065, 2017, 2066, 2434, 9535, 16255, 845...</td>\n",
       "      <td>[101, 2065, 2017, 2066, 2204, 2214, 1011, 1340...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Phil the Alien is one of those quirky films wh...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Phil alien is one of the quirky movies based o...</td>\n",
       "      <td>[101, 6316, 1996, 7344, 2003, 2028, 1997, 2216...</td>\n",
       "      <td>[101, 6316, 7344, 2003, 2028, 1997, 1996, 2186...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I saw this movie when I was about 12 when it c...</td>\n",
       "      <td>negative</td>\n",
       "      <td>I saw this movie when I was 12 years old. I th...</td>\n",
       "      <td>[101, 1045, 2387, 2023, 3185, 2043, 1045, 2001...</td>\n",
       "      <td>[101, 1045, 2387, 2023, 3185, 2043, 1045, 2001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>So im not a big fan of Boll's work but then ag...</td>\n",
       "      <td>negative</td>\n",
       "      <td>It&amp;#39;s not a big fan of Boll&amp;#39;s work, but...</td>\n",
       "      <td>[101, 2061, 10047, 2025, 1037, 2502, 5470, 199...</td>\n",
       "      <td>[101, 2009, 1004, 1001, 4464, 1025, 1055, 2025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The cast played Shakespeare.&lt;br /&gt;&lt;br /&gt;Shakes...</td>\n",
       "      <td>negative</td>\n",
       "      <td>The cast played Shakespeare. &lt;br /&gt;&lt;br /&gt; Shak...</td>\n",
       "      <td>[101, 1996, 3459, 2209, 8101, 1012, 1026, 7987...</td>\n",
       "      <td>[101, 1996, 3459, 2209, 8101, 1012, 1026, 7987...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>This a fantastic movie of three prisoners who ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>This is a fantastic movie made up of three fam...</td>\n",
       "      <td>[101, 2023, 1037, 10392, 3185, 1997, 2093, 589...</td>\n",
       "      <td>[101, 2023, 2003, 1037, 10392, 3185, 2081, 203...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Kind of drawn in by the erotic scenes, only to...</td>\n",
       "      <td>negative</td>\n",
       "      <td>The kind of thing that was drawn in the erotic...</td>\n",
       "      <td>[101, 2785, 1997, 4567, 1999, 2011, 1996, 1425...</td>\n",
       "      <td>[101, 1996, 2785, 1997, 2518, 2008, 2001, 4567...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Some films just simply should not be remade. T...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Some movies simply can not be reproduced. This...</td>\n",
       "      <td>[101, 2070, 3152, 2074, 3432, 2323, 2025, 2022...</td>\n",
       "      <td>[101, 2070, 5691, 3432, 2064, 2025, 2022, 2229...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>This movie made it into one of my top 10 most ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>This movie made it one of my top 10 scariest m...</td>\n",
       "      <td>[101, 2023, 3185, 2081, 2009, 2046, 2028, 1997...</td>\n",
       "      <td>[101, 2023, 3185, 2081, 2009, 2028, 1997, 2026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I remember this film,it was the first film i h...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I remember this movie. It was the first movie ...</td>\n",
       "      <td>[101, 1045, 3342, 2023, 2143, 1010, 2009, 2001...</td>\n",
       "      <td>[101, 1045, 3342, 2023, 3185, 1012, 2009, 2001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>An awful film! It must have been up against so...</td>\n",
       "      <td>negative</td>\n",
       "      <td>A terrible movie! In order to be nominated for...</td>\n",
       "      <td>[101, 2019, 9643, 2143, 999, 2009, 2442, 2031,...</td>\n",
       "      <td>[101, 1037, 6659, 3185, 999, 1999, 2344, 2000,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>After the success of Die Hard and it's sequels...</td>\n",
       "      <td>positive</td>\n",
       "      <td>It is also the success and successor of Die Ha...</td>\n",
       "      <td>[101, 2044, 1996, 3112, 1997, 3280, 2524, 1998...</td>\n",
       "      <td>[101, 2009, 2003, 2036, 1996, 3112, 1998, 6332...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>I had the terrible misfortune of having to vie...</td>\n",
       "      <td>negative</td>\n",
       "      <td>I had a terrible unhappiness that I had to see...</td>\n",
       "      <td>[101, 1045, 2018, 1996, 6659, 28616, 13028, 98...</td>\n",
       "      <td>[101, 1045, 2018, 1037, 6659, 4895, 3270, 9397...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What an absolutely stunning movie, if you have...</td>\n",
       "      <td>positive</td>\n",
       "      <td>It&amp;#39;s an absolutely amazing movie, and if y...</td>\n",
       "      <td>[101, 2054, 2019, 7078, 14726, 3185, 1010, 206...</td>\n",
       "      <td>[101, 2009, 1004, 1001, 4464, 1025, 1055, 2019...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>First of all, let's get a few things straight ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>First of all, here are some facts. a) I am an ...</td>\n",
       "      <td>[101, 2034, 1997, 2035, 1010, 2292, 1005, 1055...</td>\n",
       "      <td>[101, 2034, 1997, 2035, 1010, 2182, 2024, 2070...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>This was the worst movie I saw at WorldFest an...</td>\n",
       "      <td>negative</td>\n",
       "      <td>This was the worst movie I&amp;#39;ve seen at Worl...</td>\n",
       "      <td>[101, 2023, 2001, 1996, 5409, 3185, 1045, 2387...</td>\n",
       "      <td>[101, 2023, 2001, 1996, 5409, 3185, 1045, 1004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>The Karen Carpenter Story shows a little more ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Karen Carpenter Story reveals more about the c...</td>\n",
       "      <td>[101, 1996, 8129, 10533, 2466, 3065, 1037, 221...</td>\n",
       "      <td>[101, 8129, 10533, 2466, 7657, 2062, 2055, 199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>\"The Cell\" is an exotic masterpiece, a dizzyin...</td>\n",
       "      <td>positive</td>\n",
       "      <td>&amp;quot;The Cell&amp;quot; is an exotic masterpiece ...</td>\n",
       "      <td>[101, 1000, 1996, 3526, 1000, 2003, 2019, 1256...</td>\n",
       "      <td>[101, 1004, 22035, 2102, 1025, 1996, 3526, 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>This film tried to be too many things all at o...</td>\n",
       "      <td>negative</td>\n",
       "      <td>The film covered political satire, Hollywood b...</td>\n",
       "      <td>[101, 2023, 2143, 2699, 2000, 2022, 2205, 2116...</td>\n",
       "      <td>[101, 1996, 2143, 3139, 2576, 18312, 1010, 536...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>This movie was so frustrating. Everything seem...</td>\n",
       "      <td>negative</td>\n",
       "      <td>This movie was so nervous. Everything seemed e...</td>\n",
       "      <td>[101, 2023, 3185, 2001, 2061, 25198, 1012, 267...</td>\n",
       "      <td>[101, 2023, 3185, 2001, 2061, 6091, 1012, 2673...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>'War movie' is a Hollywood genre that has been...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Warfare is Hollywood&amp;#39;s genre, iterating ov...</td>\n",
       "      <td>[101, 1005, 2162, 3185, 1005, 2003, 1037, 5365...</td>\n",
       "      <td>[101, 8309, 2003, 5365, 1004, 1001, 4464, 1025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49970</th>\n",
       "      <td>This movie is a total dog. I found myself stra...</td>\n",
       "      <td>negative</td>\n",
       "      <td>This movie is a total of dogs. I did not want ...</td>\n",
       "      <td>[101, 2023, 3185, 2003, 1037, 2561, 3899, 1012...</td>\n",
       "      <td>[101, 2023, 3185, 2003, 1037, 2561, 1997, 6077...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49971</th>\n",
       "      <td>With several name actors (Lance Henrikson, Dav...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Why did Jeffery Combs lead with several actors...</td>\n",
       "      <td>[101, 2007, 2195, 2171, 5889, 1006, 9993, 1874...</td>\n",
       "      <td>[101, 2339, 2106, 5076, 7301, 22863, 2015, 259...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49972</th>\n",
       "      <td>The future of fantasy never looked so dark! Ch...</td>\n",
       "      <td>negative</td>\n",
       "      <td>The future of fantasy never looked too dark! C...</td>\n",
       "      <td>[101, 1996, 2925, 1997, 5913, 2196, 2246, 2061...</td>\n",
       "      <td>[101, 1996, 2925, 1997, 5913, 2196, 2246, 2205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49973</th>\n",
       "      <td>The title leads viewers to believe that this i...</td>\n",
       "      <td>negative</td>\n",
       "      <td>This title is a funny movie that viewers will ...</td>\n",
       "      <td>[101, 1996, 2516, 5260, 7193, 2000, 2903, 2008...</td>\n",
       "      <td>[101, 2023, 2516, 2003, 1037, 6057, 3185, 2008...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49974</th>\n",
       "      <td>For the most part, \"Michael\" is a disaster  t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>In most cases, &amp;quot;Michael&amp;quot; is a disast...</td>\n",
       "      <td>[101, 2005, 1996, 2087, 2112, 1010, 1000, 2745...</td>\n",
       "      <td>[101, 1999, 2087, 3572, 1010, 1004, 22035, 210...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49975</th>\n",
       "      <td>90 minutes of Mindy...Mindy is a tease to boyf...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Mindy is 90 minutes ... Mindy teases his Bill ...</td>\n",
       "      <td>[101, 3938, 2781, 1997, 2568, 2100, 1012, 1012...</td>\n",
       "      <td>[101, 2568, 2100, 2003, 3938, 2781, 1012, 1012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49976</th>\n",
       "      <td>I saw the movie in the theater at its release,...</td>\n",
       "      <td>positive</td>\n",
       "      <td>When I saw a movie in the theater, I saw VHS t...</td>\n",
       "      <td>[101, 1045, 2387, 1996, 3185, 1999, 1996, 4258...</td>\n",
       "      <td>[101, 2043, 1045, 2387, 1037, 3185, 1999, 1996...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49977</th>\n",
       "      <td>Dog Bite Dog isn't going to be for everyone, b...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Dog-bite dogs are not for everyone, but I real...</td>\n",
       "      <td>[101, 3899, 6805, 3899, 3475, 1005, 1056, 2183...</td>\n",
       "      <td>[101, 3899, 1011, 6805, 6077, 2024, 2025, 2005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49978</th>\n",
       "      <td>Halloween is one of those movies that gets you...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Halloween is one of the movies that brings you...</td>\n",
       "      <td>[101, 14414, 2003, 2028, 1997, 2216, 5691, 200...</td>\n",
       "      <td>[101, 14414, 2003, 2028, 1997, 1996, 5691, 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49979</th>\n",
       "      <td>I saw this with high expectations. Come on, it...</td>\n",
       "      <td>negative</td>\n",
       "      <td>I saw this with high expectations. Hey, Akshay...</td>\n",
       "      <td>[101, 1045, 2387, 2023, 2007, 2152, 10908, 101...</td>\n",
       "      <td>[101, 1045, 2387, 2023, 2007, 2152, 10908, 101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49980</th>\n",
       "      <td>A stunning film of high quality.&lt;br /&gt;&lt;br /&gt;Ap...</td>\n",
       "      <td>positive</td>\n",
       "      <td>A high quality movie. &lt;br /&gt;&lt;br /&gt; As I said c...</td>\n",
       "      <td>[101, 1037, 14726, 2143, 1997, 2152, 3737, 101...</td>\n",
       "      <td>[101, 1037, 2152, 3737, 3185, 1012, 1026, 7987...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49981</th>\n",
       "      <td>And I repeat, please do not see this movie! Th...</td>\n",
       "      <td>negative</td>\n",
       "      <td>And repeat. Please do not watch this movie. Th...</td>\n",
       "      <td>[101, 1998, 1045, 9377, 1010, 3531, 2079, 2025...</td>\n",
       "      <td>[101, 1998, 9377, 1012, 3531, 2079, 2025, 3422...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49982</th>\n",
       "      <td>To be hones, I used to like this show and watc...</td>\n",
       "      <td>negative</td>\n",
       "      <td>To be hones, I liked this show and watched reg...</td>\n",
       "      <td>[101, 2000, 2022, 10189, 2229, 1010, 1045, 210...</td>\n",
       "      <td>[101, 2000, 2022, 10189, 2229, 1010, 1045, 466...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49983</th>\n",
       "      <td>I loved it, having been a fan of the original ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I was a fan of the original series and I liked...</td>\n",
       "      <td>[101, 1045, 3866, 2009, 1010, 2383, 2042, 1037...</td>\n",
       "      <td>[101, 1045, 2001, 1037, 5470, 1997, 1996, 2434...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49984</th>\n",
       "      <td>Hello it is I Derrick Cannon and I welcome you...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Hello, this is Derek Canon. Canon Light Review...</td>\n",
       "      <td>[101, 7592, 2009, 2003, 1045, 18928, 8854, 199...</td>\n",
       "      <td>[101, 7592, 1010, 2023, 2003, 7256, 9330, 1012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49985</th>\n",
       "      <td>Imaginary Heroes is clearly the best film of t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Imaginary Heroes is definitely the best movie ...</td>\n",
       "      <td>[101, 15344, 7348, 2003, 4415, 1996, 2190, 214...</td>\n",
       "      <td>[101, 15344, 7348, 2003, 5791, 1996, 2190, 318...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49986</th>\n",
       "      <td>This movie is a disgrace to the Major League F...</td>\n",
       "      <td>negative</td>\n",
       "      <td>This film is a disgrace to the major league fr...</td>\n",
       "      <td>[101, 2023, 3185, 2003, 1037, 29591, 2000, 199...</td>\n",
       "      <td>[101, 2023, 2143, 2003, 1037, 29591, 2000, 199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49987</th>\n",
       "      <td>A remake of Alejandro Amenabar's Abre los Ojos...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Alejandro Amenabar&amp;#39;s remake of Abre los Oj...</td>\n",
       "      <td>[101, 1037, 12661, 1997, 16810, 2572, 8189, 82...</td>\n",
       "      <td>[101, 16810, 2572, 8189, 8237, 1004, 1001, 446...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49988</th>\n",
       "      <td>When I first tuned in on this morning news, I ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>When I first heard the news this morning I tho...</td>\n",
       "      <td>[101, 2043, 1045, 2034, 15757, 1999, 2006, 202...</td>\n",
       "      <td>[101, 2043, 1045, 2034, 2657, 1996, 2739, 2023...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49989</th>\n",
       "      <td>I got this one a few weeks ago and love it! It...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I got this a few weeks ago and love it! Modern...</td>\n",
       "      <td>[101, 1045, 2288, 2023, 2028, 1037, 2261, 3134...</td>\n",
       "      <td>[101, 1045, 2288, 2023, 1037, 2261, 3134, 3283...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49990</th>\n",
       "      <td>Lame, lame, lame!!! A 90-minute cringe-fest th...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Lame, lame, lame !!! A 90 minute hateful fest ...</td>\n",
       "      <td>[101, 20342, 1010, 20342, 1010, 20342, 999, 99...</td>\n",
       "      <td>[101, 20342, 1010, 20342, 1010, 20342, 999, 99...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49991</th>\n",
       "      <td>Les Visiteurs, the first movie about the medie...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Les Visiteurs, the first movie on the medieval...</td>\n",
       "      <td>[101, 4649, 3942, 26744, 1010, 1996, 2034, 318...</td>\n",
       "      <td>[101, 4649, 3942, 26744, 1010, 1996, 2034, 318...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49992</th>\n",
       "      <td>John Garfield plays a Marine who is blinded by...</td>\n",
       "      <td>positive</td>\n",
       "      <td>John Garfield is a member of the Marine Corps ...</td>\n",
       "      <td>[101, 2198, 20170, 3248, 1037, 3884, 2040, 200...</td>\n",
       "      <td>[101, 2198, 20170, 2003, 1037, 2266, 1997, 199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49993</th>\n",
       "      <td>Robert Colomb has two full-time jobs. He's kno...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Robert Colomb has 2 full-time jobs. He is know...</td>\n",
       "      <td>[101, 2728, 8902, 5358, 2497, 2038, 2048, 2440...</td>\n",
       "      <td>[101, 2728, 8902, 5358, 2497, 2038, 1016, 2440...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49994</th>\n",
       "      <td>This is your typical junk comedy.&lt;br /&gt;&lt;br /&gt;T...</td>\n",
       "      <td>negative</td>\n",
       "      <td>This is a typical junk comedy. &lt;br /&gt;&lt;br /&gt; Th...</td>\n",
       "      <td>[101, 2023, 2003, 2115, 5171, 18015, 4038, 101...</td>\n",
       "      <td>[101, 2023, 2003, 1037, 5171, 18015, 4038, 101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I thought this movie did a good job. It was no...</td>\n",
       "      <td>[101, 1045, 2245, 2023, 3185, 2106, 1037, 2091...</td>\n",
       "      <td>[101, 1045, 2245, 2023, 3185, 2106, 1037, 2204...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Bad conspiracy, bad conversation, bad acting, ...</td>\n",
       "      <td>[101, 2919, 5436, 1010, 2919, 7982, 1010, 2919...</td>\n",
       "      <td>[101, 2919, 9714, 1010, 2919, 4512, 1010, 2919...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "      <td>I am a Catholic from high school and universit...</td>\n",
       "      <td>[101, 1045, 2572, 1037, 3234, 4036, 1999, 2877...</td>\n",
       "      <td>[101, 1045, 2572, 1037, 3234, 2013, 2152, 2082...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>I must agree with the previous comments and as...</td>\n",
       "      <td>[101, 1045, 1005, 1049, 2183, 2000, 2031, 2000...</td>\n",
       "      <td>[101, 1045, 2442, 5993, 2007, 1996, 3025, 7928...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "      <td>No one expects Star Trek to be a high art, but...</td>\n",
       "      <td>[101, 2053, 2028, 24273, 1996, 2732, 10313, 56...</td>\n",
       "      <td>[101, 2053, 2028, 24273, 2732, 10313, 2000, 20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment  \\\n",
       "0      One of the other reviewers has mentioned that ...  positive   \n",
       "1      A wonderful little production. <br /><br />The...  positive   \n",
       "2      I thought this was a wonderful way to spend ti...  positive   \n",
       "3      Basically there's a family where a little boy ...  negative   \n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "5      Probably my all-time favorite movie, a story o...  positive   \n",
       "6      I sure would like to see a resurrection of a u...  positive   \n",
       "7      This show was an amazing, fresh & innovative i...  negative   \n",
       "8      Encouraged by the positive comments about this...  negative   \n",
       "9      If you like original gut wrenching laughter yo...  positive   \n",
       "10     Phil the Alien is one of those quirky films wh...  negative   \n",
       "11     I saw this movie when I was about 12 when it c...  negative   \n",
       "12     So im not a big fan of Boll's work but then ag...  negative   \n",
       "13     The cast played Shakespeare.<br /><br />Shakes...  negative   \n",
       "14     This a fantastic movie of three prisoners who ...  positive   \n",
       "15     Kind of drawn in by the erotic scenes, only to...  negative   \n",
       "16     Some films just simply should not be remade. T...  positive   \n",
       "17     This movie made it into one of my top 10 most ...  negative   \n",
       "18     I remember this film,it was the first film i h...  positive   \n",
       "19     An awful film! It must have been up against so...  negative   \n",
       "20     After the success of Die Hard and it's sequels...  positive   \n",
       "21     I had the terrible misfortune of having to vie...  negative   \n",
       "22     What an absolutely stunning movie, if you have...  positive   \n",
       "23     First of all, let's get a few things straight ...  negative   \n",
       "24     This was the worst movie I saw at WorldFest an...  negative   \n",
       "25     The Karen Carpenter Story shows a little more ...  positive   \n",
       "26     \"The Cell\" is an exotic masterpiece, a dizzyin...  positive   \n",
       "27     This film tried to be too many things all at o...  negative   \n",
       "28     This movie was so frustrating. Everything seem...  negative   \n",
       "29     'War movie' is a Hollywood genre that has been...  positive   \n",
       "...                                                  ...       ...   \n",
       "49970  This movie is a total dog. I found myself stra...  negative   \n",
       "49971  With several name actors (Lance Henrikson, Dav...  negative   \n",
       "49972  The future of fantasy never looked so dark! Ch...  negative   \n",
       "49973  The title leads viewers to believe that this i...  negative   \n",
       "49974  For the most part, \"Michael\" is a disaster  t...  negative   \n",
       "49975  90 minutes of Mindy...Mindy is a tease to boyf...  negative   \n",
       "49976  I saw the movie in the theater at its release,...  positive   \n",
       "49977  Dog Bite Dog isn't going to be for everyone, b...  positive   \n",
       "49978  Halloween is one of those movies that gets you...  positive   \n",
       "49979  I saw this with high expectations. Come on, it...  negative   \n",
       "49980  A stunning film of high quality.<br /><br />Ap...  positive   \n",
       "49981  And I repeat, please do not see this movie! Th...  negative   \n",
       "49982  To be hones, I used to like this show and watc...  negative   \n",
       "49983  I loved it, having been a fan of the original ...  positive   \n",
       "49984  Hello it is I Derrick Cannon and I welcome you...  negative   \n",
       "49985  Imaginary Heroes is clearly the best film of t...  positive   \n",
       "49986  This movie is a disgrace to the Major League F...  negative   \n",
       "49987  A remake of Alejandro Amenabar's Abre los Ojos...  negative   \n",
       "49988  When I first tuned in on this morning news, I ...  negative   \n",
       "49989  I got this one a few weeks ago and love it! It...  positive   \n",
       "49990  Lame, lame, lame!!! A 90-minute cringe-fest th...  negative   \n",
       "49991  Les Visiteurs, the first movie about the medie...  negative   \n",
       "49992  John Garfield plays a Marine who is blinded by...  positive   \n",
       "49993  Robert Colomb has two full-time jobs. He's kno...  negative   \n",
       "49994  This is your typical junk comedy.<br /><br />T...  negative   \n",
       "49995  I thought this movie did a down right good job...  positive   \n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative   \n",
       "49997  I am a Catholic taught in parochial elementary...  negative   \n",
       "49998  I'm going to have to disagree with the previou...  negative   \n",
       "49999  No one expects the Star Trek movies to be high...  negative   \n",
       "\n",
       "                                                   ko_bt  \\\n",
       "0      One of the other reviewers mentioned that you ...   \n",
       "1      Nice little production. <br /><br /> The shoot...   \n",
       "2      I thought this was a great way to spend time o...   \n",
       "3      Basically there is a family where a little boy...   \n",
       "4      Petter Mattei&#39;s &quot;Love for Money&#39;s...   \n",
       "5      Perhaps the story of the best film ever, selfl...   \n",
       "6      I would like to see the revival of the Seahunt...   \n",
       "7      This show was a stunning fresh and innovative ...   \n",
       "8      I was looking forward to seeing this movie bec...   \n",
       "9      If you like good old-fashioned laughs, you&#39...   \n",
       "10     Phil alien is one of the quirky movies based o...   \n",
       "11     I saw this movie when I was 12 years old. I th...   \n",
       "12     It&#39;s not a big fan of Boll&#39;s work, but...   \n",
       "13     The cast played Shakespeare. <br /><br /> Shak...   \n",
       "14     This is a fantastic movie made up of three fam...   \n",
       "15     The kind of thing that was drawn in the erotic...   \n",
       "16     Some movies simply can not be reproduced. This...   \n",
       "17     This movie made it one of my top 10 scariest m...   \n",
       "18     I remember this movie. It was the first movie ...   \n",
       "19     A terrible movie! In order to be nominated for...   \n",
       "20     It is also the success and successor of Die Ha...   \n",
       "21     I had a terrible unhappiness that I had to see...   \n",
       "22     It&#39;s an absolutely amazing movie, and if y...   \n",
       "23     First of all, here are some facts. a) I am an ...   \n",
       "24     This was the worst movie I&#39;ve seen at Worl...   \n",
       "25     Karen Carpenter Story reveals more about the c...   \n",
       "26     &quot;The Cell&quot; is an exotic masterpiece ...   \n",
       "27     The film covered political satire, Hollywood b...   \n",
       "28     This movie was so nervous. Everything seemed e...   \n",
       "29     Warfare is Hollywood&#39;s genre, iterating ov...   \n",
       "...                                                  ...   \n",
       "49970  This movie is a total of dogs. I did not want ...   \n",
       "49971  Why did Jeffery Combs lead with several actors...   \n",
       "49972  The future of fantasy never looked too dark! C...   \n",
       "49973  This title is a funny movie that viewers will ...   \n",
       "49974  In most cases, &quot;Michael&quot; is a disast...   \n",
       "49975  Mindy is 90 minutes ... Mindy teases his Bill ...   \n",
       "49976  When I saw a movie in the theater, I saw VHS t...   \n",
       "49977  Dog-bite dogs are not for everyone, but I real...   \n",
       "49978  Halloween is one of the movies that brings you...   \n",
       "49979  I saw this with high expectations. Hey, Akshay...   \n",
       "49980  A high quality movie. <br /><br /> As I said c...   \n",
       "49981  And repeat. Please do not watch this movie. Th...   \n",
       "49982  To be hones, I liked this show and watched reg...   \n",
       "49983  I was a fan of the original series and I liked...   \n",
       "49984  Hello, this is Derek Canon. Canon Light Review...   \n",
       "49985  Imaginary Heroes is definitely the best movie ...   \n",
       "49986  This film is a disgrace to the major league fr...   \n",
       "49987  Alejandro Amenabar&#39;s remake of Abre los Oj...   \n",
       "49988  When I first heard the news this morning I tho...   \n",
       "49989  I got this a few weeks ago and love it! Modern...   \n",
       "49990  Lame, lame, lame !!! A 90 minute hateful fest ...   \n",
       "49991  Les Visiteurs, the first movie on the medieval...   \n",
       "49992  John Garfield is a member of the Marine Corps ...   \n",
       "49993  Robert Colomb has 2 full-time jobs. He is know...   \n",
       "49994  This is a typical junk comedy. <br /><br /> Th...   \n",
       "49995  I thought this movie did a good job. It was no...   \n",
       "49996  Bad conspiracy, bad conversation, bad acting, ...   \n",
       "49997  I am a Catholic from high school and universit...   \n",
       "49998  I must agree with the previous comments and as...   \n",
       "49999  No one expects Star Trek to be a high art, but...   \n",
       "\n",
       "                                                 org_ids  \\\n",
       "0      [101, 2028, 1997, 1996, 2060, 15814, 2038, 385...   \n",
       "1      [101, 1037, 6919, 2210, 2537, 1012, 1026, 7987...   \n",
       "2      [101, 1045, 2245, 2023, 2001, 1037, 6919, 2126...   \n",
       "3      [101, 10468, 2045, 1005, 1055, 1037, 2155, 207...   \n",
       "4      [101, 9004, 3334, 4717, 7416, 1005, 1055, 1000...   \n",
       "5      [101, 2763, 2026, 2035, 1011, 2051, 5440, 3185...   \n",
       "6      [101, 1045, 2469, 2052, 2066, 2000, 2156, 1037...   \n",
       "7      [101, 2023, 2265, 2001, 2019, 6429, 1010, 4840...   \n",
       "8      [101, 6628, 2011, 1996, 3893, 7928, 2055, 2023...   \n",
       "9      [101, 2065, 2017, 2066, 2434, 9535, 16255, 845...   \n",
       "10     [101, 6316, 1996, 7344, 2003, 2028, 1997, 2216...   \n",
       "11     [101, 1045, 2387, 2023, 3185, 2043, 1045, 2001...   \n",
       "12     [101, 2061, 10047, 2025, 1037, 2502, 5470, 199...   \n",
       "13     [101, 1996, 3459, 2209, 8101, 1012, 1026, 7987...   \n",
       "14     [101, 2023, 1037, 10392, 3185, 1997, 2093, 589...   \n",
       "15     [101, 2785, 1997, 4567, 1999, 2011, 1996, 1425...   \n",
       "16     [101, 2070, 3152, 2074, 3432, 2323, 2025, 2022...   \n",
       "17     [101, 2023, 3185, 2081, 2009, 2046, 2028, 1997...   \n",
       "18     [101, 1045, 3342, 2023, 2143, 1010, 2009, 2001...   \n",
       "19     [101, 2019, 9643, 2143, 999, 2009, 2442, 2031,...   \n",
       "20     [101, 2044, 1996, 3112, 1997, 3280, 2524, 1998...   \n",
       "21     [101, 1045, 2018, 1996, 6659, 28616, 13028, 98...   \n",
       "22     [101, 2054, 2019, 7078, 14726, 3185, 1010, 206...   \n",
       "23     [101, 2034, 1997, 2035, 1010, 2292, 1005, 1055...   \n",
       "24     [101, 2023, 2001, 1996, 5409, 3185, 1045, 2387...   \n",
       "25     [101, 1996, 8129, 10533, 2466, 3065, 1037, 221...   \n",
       "26     [101, 1000, 1996, 3526, 1000, 2003, 2019, 1256...   \n",
       "27     [101, 2023, 2143, 2699, 2000, 2022, 2205, 2116...   \n",
       "28     [101, 2023, 3185, 2001, 2061, 25198, 1012, 267...   \n",
       "29     [101, 1005, 2162, 3185, 1005, 2003, 1037, 5365...   \n",
       "...                                                  ...   \n",
       "49970  [101, 2023, 3185, 2003, 1037, 2561, 3899, 1012...   \n",
       "49971  [101, 2007, 2195, 2171, 5889, 1006, 9993, 1874...   \n",
       "49972  [101, 1996, 2925, 1997, 5913, 2196, 2246, 2061...   \n",
       "49973  [101, 1996, 2516, 5260, 7193, 2000, 2903, 2008...   \n",
       "49974  [101, 2005, 1996, 2087, 2112, 1010, 1000, 2745...   \n",
       "49975  [101, 3938, 2781, 1997, 2568, 2100, 1012, 1012...   \n",
       "49976  [101, 1045, 2387, 1996, 3185, 1999, 1996, 4258...   \n",
       "49977  [101, 3899, 6805, 3899, 3475, 1005, 1056, 2183...   \n",
       "49978  [101, 14414, 2003, 2028, 1997, 2216, 5691, 200...   \n",
       "49979  [101, 1045, 2387, 2023, 2007, 2152, 10908, 101...   \n",
       "49980  [101, 1037, 14726, 2143, 1997, 2152, 3737, 101...   \n",
       "49981  [101, 1998, 1045, 9377, 1010, 3531, 2079, 2025...   \n",
       "49982  [101, 2000, 2022, 10189, 2229, 1010, 1045, 210...   \n",
       "49983  [101, 1045, 3866, 2009, 1010, 2383, 2042, 1037...   \n",
       "49984  [101, 7592, 2009, 2003, 1045, 18928, 8854, 199...   \n",
       "49985  [101, 15344, 7348, 2003, 4415, 1996, 2190, 214...   \n",
       "49986  [101, 2023, 3185, 2003, 1037, 29591, 2000, 199...   \n",
       "49987  [101, 1037, 12661, 1997, 16810, 2572, 8189, 82...   \n",
       "49988  [101, 2043, 1045, 2034, 15757, 1999, 2006, 202...   \n",
       "49989  [101, 1045, 2288, 2023, 2028, 1037, 2261, 3134...   \n",
       "49990  [101, 20342, 1010, 20342, 1010, 20342, 999, 99...   \n",
       "49991  [101, 4649, 3942, 26744, 1010, 1996, 2034, 318...   \n",
       "49992  [101, 2198, 20170, 3248, 1037, 3884, 2040, 200...   \n",
       "49993  [101, 2728, 8902, 5358, 2497, 2038, 2048, 2440...   \n",
       "49994  [101, 2023, 2003, 2115, 5171, 18015, 4038, 101...   \n",
       "49995  [101, 1045, 2245, 2023, 3185, 2106, 1037, 2091...   \n",
       "49996  [101, 2919, 5436, 1010, 2919, 7982, 1010, 2919...   \n",
       "49997  [101, 1045, 2572, 1037, 3234, 4036, 1999, 2877...   \n",
       "49998  [101, 1045, 1005, 1049, 2183, 2000, 2031, 2000...   \n",
       "49999  [101, 2053, 2028, 24273, 1996, 2732, 10313, 56...   \n",
       "\n",
       "                                                  ko_ids  \n",
       "0      [101, 2028, 1997, 1996, 2060, 15814, 3855, 200...  \n",
       "1      [101, 3835, 2210, 2537, 1012, 1026, 7987, 1013...  \n",
       "2      [101, 1045, 2245, 2023, 2001, 1037, 2307, 2126...  \n",
       "3      [101, 10468, 2045, 2003, 1037, 2155, 2073, 103...  \n",
       "4      [101, 9004, 3334, 4717, 7416, 1004, 1001, 4464...  \n",
       "5      [101, 3383, 1996, 2466, 1997, 1996, 2190, 2143...  \n",
       "6      [101, 1045, 2052, 2066, 2000, 2156, 1996, 6308...  \n",
       "7      [101, 2023, 2265, 2001, 1037, 14726, 4840, 199...  \n",
       "8      [101, 1045, 2001, 2559, 2830, 2000, 3773, 2023...  \n",
       "9      [101, 2065, 2017, 2066, 2204, 2214, 1011, 1340...  \n",
       "10     [101, 6316, 7344, 2003, 2028, 1997, 1996, 2186...  \n",
       "11     [101, 1045, 2387, 2023, 3185, 2043, 1045, 2001...  \n",
       "12     [101, 2009, 1004, 1001, 4464, 1025, 1055, 2025...  \n",
       "13     [101, 1996, 3459, 2209, 8101, 1012, 1026, 7987...  \n",
       "14     [101, 2023, 2003, 1037, 10392, 3185, 2081, 203...  \n",
       "15     [101, 1996, 2785, 1997, 2518, 2008, 2001, 4567...  \n",
       "16     [101, 2070, 5691, 3432, 2064, 2025, 2022, 2229...  \n",
       "17     [101, 2023, 3185, 2081, 2009, 2028, 1997, 2026...  \n",
       "18     [101, 1045, 3342, 2023, 3185, 1012, 2009, 2001...  \n",
       "19     [101, 1037, 6659, 3185, 999, 1999, 2344, 2000,...  \n",
       "20     [101, 2009, 2003, 2036, 1996, 3112, 1998, 6332...  \n",
       "21     [101, 1045, 2018, 1037, 6659, 4895, 3270, 9397...  \n",
       "22     [101, 2009, 1004, 1001, 4464, 1025, 1055, 2019...  \n",
       "23     [101, 2034, 1997, 2035, 1010, 2182, 2024, 2070...  \n",
       "24     [101, 2023, 2001, 1996, 5409, 3185, 1045, 1004...  \n",
       "25     [101, 8129, 10533, 2466, 7657, 2062, 2055, 199...  \n",
       "26     [101, 1004, 22035, 2102, 1025, 1996, 3526, 100...  \n",
       "27     [101, 1996, 2143, 3139, 2576, 18312, 1010, 536...  \n",
       "28     [101, 2023, 3185, 2001, 2061, 6091, 1012, 2673...  \n",
       "29     [101, 8309, 2003, 5365, 1004, 1001, 4464, 1025...  \n",
       "...                                                  ...  \n",
       "49970  [101, 2023, 3185, 2003, 1037, 2561, 1997, 6077...  \n",
       "49971  [101, 2339, 2106, 5076, 7301, 22863, 2015, 259...  \n",
       "49972  [101, 1996, 2925, 1997, 5913, 2196, 2246, 2205...  \n",
       "49973  [101, 2023, 2516, 2003, 1037, 6057, 3185, 2008...  \n",
       "49974  [101, 1999, 2087, 3572, 1010, 1004, 22035, 210...  \n",
       "49975  [101, 2568, 2100, 2003, 3938, 2781, 1012, 1012...  \n",
       "49976  [101, 2043, 1045, 2387, 1037, 3185, 1999, 1996...  \n",
       "49977  [101, 3899, 1011, 6805, 6077, 2024, 2025, 2005...  \n",
       "49978  [101, 14414, 2003, 2028, 1997, 1996, 5691, 200...  \n",
       "49979  [101, 1045, 2387, 2023, 2007, 2152, 10908, 101...  \n",
       "49980  [101, 1037, 2152, 3737, 3185, 1012, 1026, 7987...  \n",
       "49981  [101, 1998, 9377, 1012, 3531, 2079, 2025, 3422...  \n",
       "49982  [101, 2000, 2022, 10189, 2229, 1010, 1045, 466...  \n",
       "49983  [101, 1045, 2001, 1037, 5470, 1997, 1996, 2434...  \n",
       "49984  [101, 7592, 1010, 2023, 2003, 7256, 9330, 1012...  \n",
       "49985  [101, 15344, 7348, 2003, 5791, 1996, 2190, 318...  \n",
       "49986  [101, 2023, 2143, 2003, 1037, 29591, 2000, 199...  \n",
       "49987  [101, 16810, 2572, 8189, 8237, 1004, 1001, 446...  \n",
       "49988  [101, 2043, 1045, 2034, 2657, 1996, 2739, 2023...  \n",
       "49989  [101, 1045, 2288, 2023, 1037, 2261, 3134, 3283...  \n",
       "49990  [101, 20342, 1010, 20342, 1010, 20342, 999, 99...  \n",
       "49991  [101, 4649, 3942, 26744, 1010, 1996, 2034, 318...  \n",
       "49992  [101, 2198, 20170, 2003, 1037, 2266, 1997, 199...  \n",
       "49993  [101, 2728, 8902, 5358, 2497, 2038, 1016, 2440...  \n",
       "49994  [101, 2023, 2003, 1037, 5171, 18015, 4038, 101...  \n",
       "49995  [101, 1045, 2245, 2023, 3185, 2106, 1037, 2204...  \n",
       "49996  [101, 2919, 9714, 1010, 2919, 4512, 1010, 2919...  \n",
       "49997  [101, 1045, 2572, 1037, 3234, 2013, 2152, 2082...  \n",
       "49998  [101, 1045, 2442, 5993, 2007, 1996, 3025, 7928...  \n",
       "49999  [101, 2053, 2028, 24273, 2732, 10313, 2000, 20...  \n",
       "\n",
       "[50000 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ids_to_tensor(lst):\n",
    "    org_id_lst = [eval(i)[:512] for i in lst]\n",
    "    org_ids = pad_sequences(org_id_lst,maxlen=512,padding='post')\n",
    "    org_tensor = torch.from_numpy(org_ids)\n",
    "    return org_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(data.sentiment.replace({'negative':0,'positive':1}).values).type(torch.float32).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_tensor = convert_ids_to_tensor(data.org_ids.values)\n",
    "trans_tensor = convert_ids_to_tensor(data.ko_ids.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_td = TensorDataset(org_tensor[:20].type(torch.int64),y[:20])\n",
    "aux_td = TensorDataset(org_tensor.type(torch.int64),trans_tensor.type(torch.int64),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_td = TensorDataset(org_tensor[-10000:].type(torch.int64),y[-10000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixlengthSampler(Sampler):\n",
    "    def __init__(self, data_source, length=None):\n",
    "        self.data_source = data_source\n",
    "        self.length = length if length is not None else len(self.data_source)\n",
    "    def __iter__(self,):\n",
    "        return iter(torch.randint(low=0,high=len(self.data_source),size=(self.length,)))\n",
    "    def __len__(self): \n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = 10\n",
    "aux_batch_size = 8\n",
    "training_batch_size = 16\n",
    "test_batch_size = 16\n",
    "\n",
    "training_sampler = FixlengthSampler(training_td,length=training_steps*training_batch_size)\n",
    "training_dl = DataLoader(training_td,batch_size=training_batch_size,sampler=training_sampler)\n",
    "\n",
    "aux_sampler = FixlengthSampler(aux_td,length=training_steps*aux_batch_size)\n",
    "aux_dl = DataLoader(aux_td,batch_size=aux_batch_size,sampler=aux_sampler)\n",
    "\n",
    "test_dl = DataLoader(test_td,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "### quick test\n",
    "aux_iter = iter(aux_dl)\n",
    "bax1,bax2,bay = next(aux_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div(bl1,bl2):\n",
    "    bl1_prob,bl2_prob = torch.sigmoid(bl1), torch.sigmoid(bl2)\n",
    "    return (bl1_prob*(bl1_prob/bl2_prob).log()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0288)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the same example in wiki\n",
    "P = torch.Tensor([0.36, 0.48, 0.16])\n",
    "Q = torch.Tensor([0.333, 0.333, 0.333])\n",
    "(P * (P / Q).log()).mean()\n",
    "# tensor(0.0863), 10.2 µs ± 508\n",
    "F.kl_div(Q.log(), P, None, None, 'batchmean')\n",
    "# tensor(0.0863), 14.1 µs ± 408 ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_sig(x):\n",
    "    return torch.log(x/(1-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0288)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_div(inverse_sig(P),inverse_sig(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1186)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_div(torch.tensor(1.9),torch.tensor(5.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1186)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.kl_div(torch.tensor(5.8).sigmoid().log(), torch.tensor(1.9).sigmoid(), reduction='batchmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = BertAdam(model.parameters(),\n",
    "               lr=2e-5,\n",
    "               warmup=0.02, \n",
    "               t_total=training_steps,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "    acc = 0.0\n",
    "    for bx,by in test_dl:\n",
    "        attention_mask = bx > 0 \n",
    "        bx,by = bx.to(device),by.to(device)\n",
    "        bl = model(bx,attention_mask=attention_mask)\n",
    "        bacc = ((bl >= 0).astype(torch.float32) == by).sum()\n",
    "        acc += bacc\n",
    "    acc = acc/len(text_dl)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tsa_loss() missing 1 required positional argument: 'ita'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-294-d15232762bdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsa_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0maux_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkl_div\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlamda\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maux_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tsa_loss() missing 1 required positional argument: 'ita'"
     ]
    }
   ],
   "source": [
    "uda = True\n",
    "lamda = 0.2\n",
    "training_iter = iter(training_dl)\n",
    "aux_iter = iter(aux_dl)\n",
    "# eval_per_steps = int(training_steps/10)\n",
    "eval_per_steps = 100\n",
    "\n",
    "model = model.train()\n",
    "for i in range(training_steps):\n",
    "    bx,by = next(training_iter)\n",
    "    bx,by = bx.to(device),by.to(device)\n",
    "    \n",
    "    if uda: \n",
    "        bax1,bax2,bay = next(aux_iter)\n",
    "        bax1,bax2,bay = bax1.to(device),bax2.to(device),bay.to(device)\n",
    "        \n",
    "    progress = torch.tensor(i/training_steps)\n",
    "    ita = cal_ita(progress)\n",
    "    \n",
    "    attention_mask = bx > 0\n",
    "    bl = model(bx,attention_mask=attention_mask)\n",
    "    if uda:\n",
    "        bal1,bal2 = model(bax1),model(bax2)\n",
    "        \n",
    "        loss = tsa_loss(bl,by,ita)\n",
    "        aux_loss = kl_div(bal1,bal2)\n",
    "        loss = loss + lamda * aux_loss\n",
    "    else:\n",
    "#         loss = tsa_loss(bl,by)\n",
    "        loss = torch.binary_cross_entropy_with_logits(bl,by)\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "#     loss =  \n",
    "    if i % eval_per_steps == 0:\n",
    "        model = model.eval()\n",
    "        acc = eval_model(model)\n",
    "        model = model.train()\n",
    "        \n",
    "    break\n",
    "    \n",
    "    print(ita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7853, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = tsa_loss(bl,by,ita)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0634, grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_loss = kl_div(bl,)\n",
    "aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.kl_div(by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0854],\n",
       "        [0.2212],\n",
       "        [0.2692],\n",
       "        [0.2029],\n",
       "        [0.1858],\n",
       "        [0.2984],\n",
       "        [0.1517],\n",
       "        [0.2177],\n",
       "        [0.1325],\n",
       "        [0.1521],\n",
       "        [0.2172],\n",
       "        [0.3126],\n",
       "        [0.3012],\n",
       "        [0.0492],\n",
       "        [0.3645],\n",
       "        [0.2692]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.tsa_loss(logits, labels, ita)>"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsa_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0863)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(P * (P / Q).log()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., -3.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.tensor([[10.,-3.]])\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.tensor([[1,0]])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsa_loss(logits,labels,ita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0474]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.sigmoid(logits)\n",
    "probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.9526]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confi_probs = 1 - torch.abs(probs - labels)\n",
    "confi_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.9526]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confi_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks = (confi_probs <= ita).type(torch.float32)\n",
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(probs,labels,reduction='none') * masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.5418e-05, 4.8587e-02]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy_with_logits(logits,labels,reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsa_loss(logits,labels,ita):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    confi_probs = 1 - torch.abs(probs - labels)\n",
    "    masks = (confi_probs <= ita).type(torch.float32)\n",
    "#     print(masks)\n",
    "    loss = torch.mean(F.binary_cross_entropy(probs,labels,reduction='none') * masks)\n",
    "    loss = 1/masks.mean()*loss\n",
    "    return loss\n",
    "def cal_ita(progress):\n",
    "    return torch.exp((progress-1)*5)*(1-1/2)+1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.0486)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsa_loss(logits,labels.type(torch.float32),ita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0]])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.,  3.]])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
